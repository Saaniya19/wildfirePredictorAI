{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6FKnhj3ujL571Ez+qvbJV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saaniya19/wildfirePredictorAI/blob/main/wildfirePredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "q_Ro4ujUjENL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0914142e-53a3-49aa-e515-5f85d5386d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total wildfire training images: 7831\n",
            "total non-wildfire training images: 7401\n",
            "total wildfire validation images: 1741\n",
            "total non-wildfire validation images: 1001\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import zipfile   # so we can unzip the files\n",
        "\n",
        "local_zip = '/tmp/wildfire_data.zip'\n",
        "# 'r' makes it a read-only file\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "# unzipping the files\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()\n",
        "\n",
        "# defining the locations of the training and validation datasets\n",
        "wildfire_train = os.path.join('/tmp/wildfire_data/train/wildfire')\n",
        "nowildfire_train = os.path.join('/tmp/wildfire_data/train/nowildfire')\n",
        "wildfire_validation = os.path.join('/tmp/wildfire_data/valid/wildfire')\n",
        "nowildfire_validation = os.path.join('/tmp/wildfire_data/valid/nowildfire')\n",
        "\n",
        "# printing number of images in each training and validation dataset\n",
        "print('total wildfire training images:', len(os.listdir(wildfire_train)))\n",
        "print('total non-wildfire training images:', len(os.listdir(nowildfire_train)))\n",
        "print('total wildfire validation images:', len(os.listdir(wildfire_validation)))\n",
        "print('total non-wildfire validation images:', len(os.listdir(nowildfire_validation)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras.preprocessing\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# defining training data directory\n",
        "TRAINING_DIR = \"/tmp/wildfire_data/train/\"\n",
        "# generating more training data by zooming images\n",
        "training_datagen = ImageDataGenerator(zoom_range=0.2)\n",
        "\n",
        "# defining validation data directory\n",
        "VALIDATION_DIR = \"/tmp/wildfire_data/valid/\"\n",
        "# generating more validation data by zooming images\n",
        "validation_datagen = ImageDataGenerator(zoom_range=0.2)\n",
        "\n",
        "# defining training dataset\n",
        "# image size is 100 x 100 pixels\n",
        "# we can set class mode to categorical even though we only have two data classifications\n",
        "# batch size is 500 meaning the number of training images utilized in a single iteration of the training algorithm\n",
        "train_data = training_datagen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size = (100,100),\n",
        "    class_mode = 'categorical',\n",
        "    batch_size = 500\n",
        ")\n",
        "\n",
        "# defining validation dataset\n",
        "valid_data = validation_datagen.flow_from_directory(\n",
        "    VALIDATION_DIR,\n",
        "    target_size = (100,100),\n",
        "    class_mode = 'categorical',\n",
        "    batch_size = 500,\n",
        ")"
      ],
      "metadata": {
        "id": "aobRH8oq0GCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13ed738-e7bb-4017-84ec-4d53e860c5d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15230 images belonging to 2 classes.\n",
            "Found 2740 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageFile\n",
        "\n",
        "# the following line is so that we don't get an image truncated error\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # the input shape is a 100 x 100 pixels image with 3 bytes color\n",
        "    # this is the first convolution (with Max Pooling)\n",
        "    # output shape is 2 because there are 2 classifications: wildfire and no wildfire\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(100, 100, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # the second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # the third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # the fourth convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # flatten the results to feed into a DNN\n",
        "    # dropout helps to improve the efficiency of the NN by throwing away unnescessary neurons\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # two dense layers in NN\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # 2 because there are 2 classifications (wildfire and no wildfire)\n",
        "    tf.keras.layers.Dense(2, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data, epochs=5, steps_per_epoch=10, validation_data = valid_data, verbose = 1, validation_steps=1)\n",
        "\n",
        "model.save(\"wildfire.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUVqOJ7nJcKc",
        "outputId": "625b7082-d5cf-4be9-d5ac-670abe68318d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_24 (Conv2D)          (None, 98, 98, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_24 (MaxPoolin  (None, 49, 49, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 47, 47, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_25 (MaxPoolin  (None, 23, 23, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 21, 21, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_26 (MaxPoolin  (None, 10, 10, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " max_pooling2d_27 (MaxPoolin  (None, 4, 4, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 512)               1049088   \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,310,274\n",
            "Trainable params: 1,310,274\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "10/10 [==============================] - 242s 24s/step - loss: 23.7051 - accuracy: 0.4942 - val_loss: 0.6916 - val_accuracy: 0.3740\n",
            "Epoch 2/5\n",
            "10/10 [==============================] - 230s 23s/step - loss: 0.6482 - accuracy: 0.6438 - val_loss: 0.5548 - val_accuracy: 0.8340\n",
            "Epoch 3/5\n",
            "10/10 [==============================] - 219s 22s/step - loss: 0.8186 - accuracy: 0.6168 - val_loss: 1.1115 - val_accuracy: 0.3800\n",
            "Epoch 4/5\n",
            "10/10 [==============================] - 227s 23s/step - loss: 0.7042 - accuracy: 0.5428 - val_loss: 0.6068 - val_accuracy: 0.6760\n",
            "Epoch 5/5\n",
            "10/10 [==============================] - 223s 23s/step - loss: 0.7874 - accuracy: 0.6236 - val_loss: 0.9600 - val_accuracy: 0.4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "import keras.utils as image\n",
        "import os\n",
        "from os import listdir\n",
        "\n",
        "pathFire = \"/tmp/wildfire_data/test/wildfire/\"\n",
        "pathNoFire = \"/tmp/wildfire_data/test/nowildfire/\"\n",
        "total = 0\n",
        "\n",
        "img = image.load_img(pathFire+\"-63.80422,51.85132.jpg\", target_size=(100, 100))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "images = np.vstack([x])\n",
        "classes = model.predict(images, batch_size=1)\n",
        "print(classes)\n",
        "\n",
        "totalFire = 0\n",
        "for i in os.listdir(pathFire):\n",
        "  total += 1\n",
        "  img = image.load_img(pathFire+i, target_size=(100, 100))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=1)\n",
        "  print(classes)\n",
        "  if classes[0][0]  < 0.95:\n",
        "    totalFire += 1\n",
        "  if total == 10:\n",
        "    break\n",
        "\n",
        "total = 0\n",
        "totalNoFire = 0\n",
        "for i in os.listdir(pathNoFire):\n",
        "  total += 1\n",
        "  img = image.load_img(pathNoFire+i, target_size=(100, 100))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=1)\n",
        "  print(classes)\n",
        "  if classes[0][0]  > 0.95:\n",
        "    totalNoFire += 1\n",
        "  if total == 10:\n",
        "    break\n",
        "\n",
        "print(str(totalFire) + \"/10 photos of NO wildfires were predicted correctly\")\n",
        "print(str(totalNoFire) + \"/10 photos of wildfires were predicted correctly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYXrrrMGLfYT",
        "outputId": "b5feefb1-2857-4a0e-ccff-fc5ace2e3f85"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n",
            "[[0.720066   0.26950195]]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "[[0.8001548  0.17070448]]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "[[0.6738881  0.35178018]]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "[[0.9307071  0.06519295]]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "[[0.55712277 0.43958554]]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "[[0.70348185 0.30295342]]\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "[[0.8799033  0.12098499]]\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "[[0.820153   0.17185427]]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "[[0.85624146 0.134844  ]]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "[[0.8412399  0.15961227]]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "[[0.659602   0.32348153]]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "[[0.9833637  0.01899157]]\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "[[0.9627257 0.0372322]]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "[[0.989838   0.01142697]]\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "[[0.9921128  0.00912242]]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "[[0.93651885 0.08170763]]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "[[0.9151264  0.08649848]]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "[[0.9967476  0.00345334]]\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "[[0.9927989 0.008034 ]]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "[[0.9717866  0.03454844]]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "[[0.98463887 0.01480885]]\n",
            "10/10 photos of NO wildfires were predicted correctly\n",
            "8/10 photos of wildfires were predicted correctly\n"
          ]
        }
      ]
    }
  ]
}